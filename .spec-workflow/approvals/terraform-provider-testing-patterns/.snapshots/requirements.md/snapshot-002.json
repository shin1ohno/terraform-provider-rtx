{
  "id": "snapshot_1769256727992_g9ur400w8",
  "approvalId": "approval_1769256687397_4yta7hmiu",
  "approvalTitle": "Requirements: Terraform Provider Testing Patterns (Updated)",
  "version": 2,
  "timestamp": "2026-01-24T12:12:07.991Z",
  "trigger": "approved",
  "status": "pending",
  "content": "# Requirements Document: Terraform Provider Testing Patterns\n\n## Introduction\n\nThis specification establishes comprehensive testing patterns and best practices for the Terraform provider for RTX routers. The goal is to ensure provider reliability, prevent regressions, and catch common issues such as perpetual diffs, state inconsistencies, and import failures before they reach users.\n\nThese patterns are based on HashiCorp's official recommendations (2025-2026) and lessons learned from mature providers like AWS and GCP.\n\n## Alignment with Product Vision\n\nThis feature supports the product vision by:\n- **Reliability**: Ensuring users can trust the provider to manage their critical network infrastructure\n- **Maintainability**: Establishing patterns that make it easier to add new resources with confidence\n- **Quality**: Catching bugs early in development through comprehensive test coverage\n\n## Requirements\n\n### Requirement 1: Perpetual Diff Prevention Tests\n\n**User Story:** As a provider developer, I want tests that verify resources don't produce unexpected diffs on re-apply, so that users don't see confusing \"changes\" when nothing has actually changed.\n\n#### Acceptance Criteria\n\n1. WHEN a resource is created and `terraform plan` is run again without config changes THEN the system SHALL report no changes (empty plan)\n2. WHEN a resource has Optional+Computed fields not specified in config THEN the system SHALL preserve state values without showing diffs\n3. IF a resource uses semantic equality for custom types (e.g., JSON normalization) THEN tests SHALL verify equivalent values don't produce diffs\n4. WHEN testing for empty plans THEN the system SHALL use `plancheck.ExpectEmptyPlan()` or equivalent SDK v2 pattern\n\n### Requirement 2: State Consistency Tests\n\n**User Story:** As a provider developer, I want tests that verify state matches the actual resource configuration, so that Terraform state accurately reflects reality.\n\n#### Acceptance Criteria\n\n1. WHEN a resource is created THEN the Read function SHALL populate all Computed fields in state\n2. WHEN a resource is updated THEN state values SHALL reflect the actual post-update configuration\n3. IF a discrepancy exists between state and actual resource THEN tests SHALL detect and fail\n4. WHEN using Optional+Computed fields THEN tests SHALL verify state contains either user-specified or router-returned values\n\n### Requirement 3: Import Testing\n\n**User Story:** As a provider developer, I want tests that verify resources can be imported correctly, so that users can adopt existing infrastructure into Terraform management.\n\n#### Acceptance Criteria\n\n1. WHEN a resource is imported THEN the system SHALL populate state with all current attribute values\n2. WHEN imported state is used for a subsequent plan THEN the system SHALL show no changes (if config matches)\n3. FOR each resource type THEN at least one import test SHALL exist\n4. WHEN import ID format changes THEN tests SHALL verify backward compatibility or clear migration path\n\n### Requirement 4: Plan Check Patterns\n\n**User Story:** As a provider developer, I want structured plan checks that verify resource actions and values, so that I can assert precise behavior in tests.\n\n#### Acceptance Criteria\n\n1. WHEN testing resource creation THEN tests SHALL verify `plan.ResourceActionCreate`\n2. WHEN testing resource updates THEN tests SHALL verify `plan.ResourceActionUpdate`\n3. WHEN testing resource deletion THEN tests SHALL verify `plan.ResourceActionDestroy`\n4. WHEN testing no-op scenarios THEN tests SHALL verify `plan.ResourceActionNoop`\n5. WHEN specific attribute values are expected THEN tests SHALL use `plancheck.ExpectKnownValue()`\n\n### Requirement 5: Acceptance Test Structure\n\n**User Story:** As a provider developer, I want a consistent test structure across all resources, so that tests are easy to write, understand, and maintain.\n\n#### Acceptance Criteria\n\n1. WHEN creating acceptance tests THEN the system SHALL follow the pattern: Basic → Update → Import → Error cases\n2. WHEN tests require real router access THEN PreCheck functions SHALL verify connectivity and credentials\n3. WHEN tests create resources THEN CheckDestroy functions SHALL verify cleanup\n4. WHEN tests use random values THEN acctest helpers or equivalent SHALL be used for uniqueness\n\n### Requirement 6: Value Comparison and State Checks\n\n**User Story:** As a provider developer, I want to verify that specific values in state match expected values, so that I can catch subtle bugs in value handling.\n\n#### Acceptance Criteria\n\n1. WHEN testing computed values (IDs, timestamps) THEN tests SHALL use `statecheck.ExpectKnownValue()`\n2. WHEN testing value stability across steps THEN tests SHALL use Value Comparers\n3. IF a value should remain constant after updates THEN tests SHALL verify with `compare.ValuesSame()`\n4. IF a value should change after updates THEN tests SHALL verify with `compare.ValuesDiffer()`\n\n### Requirement 7: Test Helpers and Utilities\n\n**User Story:** As a provider developer, I want reusable test helpers that reduce boilerplate, so that writing new tests is fast and consistent.\n\n#### Acceptance Criteria\n\n1. WHEN multiple tests need similar configurations THEN shared config builder functions SHALL be available\n2. WHEN tests need to verify specific RTX router behavior THEN RTX-specific assertion helpers SHALL be available\n3. WHEN tests run in parallel THEN resource names SHALL be unique to avoid conflicts\n4. WHEN test fixtures are needed THEN they SHALL be stored in `testdata/` directories\n\n### Requirement 8: State Migration Tests\n\n**User Story:** As a provider developer, I want tests that verify state upgrades work correctly when schema versions change, so that users can safely upgrade the provider without losing state.\n\n#### Acceptance Criteria\n\n1. WHEN SchemaVersion is incremented THEN unit tests SHALL verify StateUpgrader functions\n2. WHEN a StateUpgrader is added THEN tests SHALL verify transformation from old format to new format\n3. WHEN testing state migration THEN acceptance tests SHALL use ExternalProviders to apply with old provider version\n4. WHEN testing cross-version upgrade THEN the system SHALL verify empty plan after upgrade (no unexpected changes)\n5. WHEN StateUpgrader handles multiple versions THEN each version path SHALL be tested\n6. WHEN state migration fails THEN clear error messages SHALL be provided\n\n### Requirement 9: WriteOnly and Sensitive Attribute Tests\n\n**User Story:** As a provider developer, I want tests that verify sensitive and write-only attributes are handled correctly, so that credentials are not exposed in state or logs.\n\n#### Acceptance Criteria\n\n1. WHEN a WriteOnly attribute is set THEN tests SHALL verify the value is NOT stored in state\n2. WHEN a Sensitive attribute is used THEN tests SHALL verify the value is masked in plan output\n3. WHEN testing password fields THEN tests SHALL verify the password is sent to router but not readable from state\n4. WHEN updating a WriteOnly attribute THEN tests SHALL verify the update is applied without exposing the value\n5. WHEN importing a resource with WriteOnly attributes THEN tests SHALL verify appropriate handling (null or require re-specification)\n\n## Non-Functional Requirements\n\n### Code Architecture and Modularity\n\n- **Single Responsibility Principle**: Test helpers should have focused, specific purposes\n- **Modular Design**: Shared test utilities should be in `internal/provider/acctest/` or similar\n- **Dependency Management**: Test code should not leak into production builds\n- **Clear Interfaces**: Test helper functions should have self-documenting signatures\n\n### Performance\n\n- Tests should leverage `t.Parallel()` where safe\n- Test suites should complete within reasonable CI time limits\n- Network-dependent tests should use appropriate timeouts\n\n### Reliability\n\n- Tests should be deterministic (no flaky tests)\n- Tests should clean up resources even on failure\n- Tests should handle router connectivity issues gracefully\n\n### Maintainability\n\n- Each resource should have its own test file (`resource_*_test.go`)\n- Test code should follow the same style guidelines as production code\n- Complex test scenarios should be documented with comments\n\n## Scope\n\n### Included Resources for Initial Implementation\n\nThe following resources will receive comprehensive test coverage first:\n\n- [ ] rtx_admin_user (as reference implementation)\n- [ ] rtx_admin\n- [ ] Resources with Optional+Computed fields (identified in optional-field-preservation spec)\n\n### Test Categories to Implement\n\n1. **Unit Tests**: Field helpers, parsers, command builders\n2. **Acceptance Tests**: Full CRUD lifecycle with real router\n3. **Regression Tests**: Specific bug prevention (e.g., administrator lockout)\n\n## References\n\n- HashiCorp Testing Patterns Guide (2025-2026)\n- terraform-plugin-testing v1.14+ documentation\n- terraform-plugin-sdk/v2 testing documentation\n- Existing `optional-field-preservation` spec for related patterns\n",
  "fileStats": {
    "size": 8995,
    "lines": 170,
    "lastModified": "2026-01-24T12:11:17.577Z"
  },
  "comments": []
}